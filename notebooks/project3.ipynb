{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGzDNElOzhq"
      },
      "source": [
        "# DSC106 - Project 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import gcsfs\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Processes the data from the CMIP6 dataset into a CSV for our D3 visualizations.\n",
        "\n",
        "**Note:** This cell takes a long time to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting data processing for d3.js visualizations...\n",
            "Loaded data.csv index.\n",
            "\n",
            "--- Processing: historical ---\n",
            "Found zstore: gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r4i1p1f1/Amon/tas/gn/v20190308/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Processing: ssp245 ---\n",
            "Found zstore: gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp245/r4i1p1f1/Amon/tas/gn/v20200528/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Processing: ssp585 ---\n",
            "Found zstore: gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp585/r4i1p1f1/Amon/tas/gn/v20200528/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Concatenating all results ---\n",
            "\n",
            "✅ Success! Data processed and saved to 'us_regional_july_temps.csv'.\n",
            "Final DataFrame head:\n",
            "   year  july_temp_c     region    scenario\n",
            "0  1850    20.817801  Northeast  historical\n",
            "1  1851    20.774825  Northeast  historical\n",
            "2  1852    22.183362  Northeast  historical\n",
            "3  1853    21.646311  Northeast  historical\n",
            "4  1854    21.980632  Northeast  historical\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message='Sending large graph to Dask')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='Mean of empty slice')\n",
        "\n",
        "print(\"Starting data processing for d3.js visualizations...\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# We'll use 'r4i1p1f1', a variant common to all three experiments for the CESM2 model\n",
        "MEMBER_ID = 'r4i1p1f1' \n",
        "SOURCE_ID = 'CESM2'\n",
        "TABLE_ID = 'Amon'\n",
        "VARIABLE_ID = 'tas'\n",
        "EXPERIMENTS = ['historical', 'ssp245', 'ssp585'] # Baseline, medium-future, high-emissions-future\n",
        "\n",
        "# Define US regions. Longitude is 0-360 in these models.\n",
        "# W longitude = 360 - W.\n",
        "REGIONS = {\n",
        "    'Northeast': {'lat': slice(39, 48), 'lon': slice(360-81, 360-67)}, # 279-293\n",
        "    'Southeast': {'lat': slice(25, 39), 'lon': slice(360-95, 360-75)}, # 265-285\n",
        "    'Midwest':   {'lat': slice(37, 49), 'lon': slice(360-104, 360-81)},# 256-279\n",
        "    'West':      {'lat': slice(32, 49), 'lon': slice(360-125, 360-104)} # 235-256\n",
        "}\n",
        "# ---------------------\n",
        "\n",
        "try:\n",
        "    # Load the main dataframe\n",
        "    df = pd.read_csv('../data/data.csv', index_col=0)\n",
        "    print(\"Loaded data.csv index.\")\n",
        "\n",
        "    # Initialize GCS FileSystem (anonymous access)\n",
        "    gcs = gcsfs.GCSFileSystem(token='anon')\n",
        "\n",
        "    all_regional_data = []\n",
        "\n",
        "    # Loop over each experiment\n",
        "    for exp in EXPERIMENTS:\n",
        "        print(f\"\\n--- Processing: {exp} ---\")\n",
        "        \n",
        "        # Find the zstore URL for this specific experiment and member_id\n",
        "        query = (\n",
        "            f\"source_id == '{SOURCE_ID}' & \"\n",
        "            f\"member_id == '{MEMBER_ID}' & \"\n",
        "            f\"experiment_id == '{exp}' & \"\n",
        "            f\"table_id == '{TABLE_ID}' & \"\n",
        "            f\"variable_id == '{VARIABLE_ID}'\"\n",
        "        )\n",
        "        \n",
        "        df_exp = df.query(query)\n",
        "        \n",
        "        if df_exp.empty:\n",
        "            print(f\"Warning: No data found for query: {query}\")\n",
        "            continue\n",
        "            \n",
        "        zstore = df_exp.iloc[0]['zstore']\n",
        "        print(f\"Found zstore: {zstore}\")\n",
        "\n",
        "        # Open the dataset\n",
        "        try:\n",
        "            mapper = gcs.get_mapper(zstore)\n",
        "            ds = xr.open_zarr(mapper, consolidated=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening zarr store {zstore}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Loop over each region\n",
        "        for region_name, region_box in REGIONS.items():\n",
        "            print(f\"Processing region: {region_name}\")\n",
        "            \n",
        "            try:\n",
        "                # 1. Select the region\n",
        "                ds_region = ds.sel(lat=region_box['lat'], lon=region_box['lon'])\n",
        "                \n",
        "                # 2. Create latitude weights for accurate averaging\n",
        "                weights = np.cos(np.deg2rad(ds_region.lat))\n",
        "                weights.name = 'weights'\n",
        "                \n",
        "                # 3. Calculate the weighted spatial mean\n",
        "                ds_weighted_mean = ds_region.weighted(weights).mean(dim=['lat', 'lon'])\n",
        "                \n",
        "                # 4. Select only July data (month == 7) as proxy for \"extreme\" heat\n",
        "                ds_july = ds_weighted_mean.sel(time=ds_weighted_mean.time.dt.month == 7)\n",
        "                \n",
        "                # 5. Convert from Kelvin to Celsius\n",
        "                temp_c = ds_july['tas'] - 273.15\n",
        "                \n",
        "                # 6. Trigger computation\n",
        "                print(\"Computing values...\")\n",
        "                temp_c_computed = temp_c.compute()\n",
        "                print(\"...computation complete.\")\n",
        "\n",
        "                # 7. Convert to Pandas DataFrame\n",
        "                df_temp = temp_c_computed.to_dataframe()\n",
        "                \n",
        "                if df_temp.empty:\n",
        "                    print(f\"Warning: No data after processing for {region_name}, {exp}\")\n",
        "                    continue\n",
        "\n",
        "                # 8. Clean up the DataFrame\n",
        "                df_temp['year'] = df_temp.index.year\n",
        "                df_temp = df_temp.reset_index(drop=True)[['year', 'tas']]\n",
        "                df_temp = df_temp.rename(columns={'tas': 'july_temp_c'})\n",
        "                df_temp['region'] = region_name\n",
        "                df_temp['scenario'] = exp\n",
        "                \n",
        "                # 9. Append to our master list\n",
        "                all_regional_data.append(df_temp)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {region_name} for {exp}: {e}\")\n",
        "\n",
        "        # Close the dataset\n",
        "        ds.close()\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    if all_regional_data:\n",
        "        print(\"\\n--- Concatenating all results ---\")\n",
        "        final_df = pd.concat(all_regional_data, ignore_index=True)\n",
        "        \n",
        "        # Save to CSV\n",
        "        output_filename = 'us_regional_july_temps.csv'\n",
        "        final_df.to_csv(output_filename, index=False)\n",
        "        \n",
        "        print(f\"\\n✅ Success! Data processed and saved to '{output_filename}'.\")\n",
        "        print(\"Final DataFrame head:\")\n",
        "        print(final_df.head())\n",
        "    else:\n",
        "        print(\"\\nNo data was processed. Output file not created.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found. Make sure it's in the same directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
