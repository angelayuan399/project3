{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGzDNElOzhq"
      },
      "source": [
        "# DSC106 - Project 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import gcsfs\n",
        "import numpy as np\n",
        "import sys\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Processes the data from the CMIP6 dataset into a CSV for our D3 visualizations.\n",
        "\n",
        "**Note:** This cell takes a long time to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://cdn.jsdelivr.net/npm/us-atlas@3/states-albers-10m.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1029\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1402\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1409\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
            "File \u001b[1;32mc:\\Users\\justi\\miniforge3\\envs\\dsc\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "pd.read_json('https://cdn.jsdelivr.net/npm/us-atlas@3/states-albers-10m.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting data processing for d3.js visualizations...\n",
            "Loaded data.csv index.\n",
            "\n",
            "--- Processing: historical ---\n",
            "Found zstore: gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r4i1p1f1/Amon/tas/gn/v20190308/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Processing: ssp245 ---\n",
            "Found zstore: gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp245/r4i1p1f1/Amon/tas/gn/v20200528/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Processing: ssp585 ---\n",
            "Found zstore: gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp585/r4i1p1f1/Amon/tas/gn/v20200528/\n",
            "Processing region: Northeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Southeast\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: Midwest\n",
            "Computing values...\n",
            "...computation complete.\n",
            "Processing region: West\n",
            "Computing values...\n",
            "...computation complete.\n",
            "\n",
            "--- Concatenating all results ---\n",
            "\n",
            "✅ Success! Data processed and saved to 'us_regional_july_temps.csv'.\n",
            "Final DataFrame head:\n",
            "   year  july_temp_c     region    scenario\n",
            "0  1850    20.817801  Northeast  historical\n",
            "1  1851    20.774825  Northeast  historical\n",
            "2  1852    22.183362  Northeast  historical\n",
            "3  1853    21.646311  Northeast  historical\n",
            "4  1854    21.980632  Northeast  historical\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message='Sending large graph to Dask')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='Mean of empty slice')\n",
        "\n",
        "print(\"Starting data processing for d3.js visualizations...\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# We'll use 'r4i1p1f1', a variant common to all three experiments for the CESM2 model\n",
        "MEMBER_ID = 'r4i1p1f1' \n",
        "SOURCE_ID = 'CESM2'\n",
        "TABLE_ID = 'Amon'\n",
        "VARIABLE_ID = 'tas'\n",
        "EXPERIMENTS = ['historical', 'ssp245', 'ssp585'] # Baseline, medium-future, high-emissions-future\n",
        "\n",
        "# Define US regions. Longitude is 0-360 in these models.\n",
        "# W longitude = 360 - W.\n",
        "REGIONS = {\n",
        "    'Northeast': {'lat': slice(39, 48), 'lon': slice(360-81, 360-67)}, # 279-293\n",
        "    'Southeast': {'lat': slice(25, 39), 'lon': slice(360-95, 360-75)}, # 265-285\n",
        "    'Midwest':   {'lat': slice(37, 49), 'lon': slice(360-104, 360-81)},# 256-279\n",
        "    'West':      {'lat': slice(32, 49), 'lon': slice(360-125, 360-104)} # 235-256\n",
        "}\n",
        "# ---------------------\n",
        "\n",
        "try:\n",
        "    # Load the main dataframe\n",
        "    # df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv', index_col=0)\n",
        "    df = pd.read_csv('../data/data.csv', index_col=0) # data.csv is the CMIP6 data as a CSV\n",
        "    print(\"Loaded data.csv index.\")\n",
        "\n",
        "    # Initialize GCS FileSystem (anonymous access)\n",
        "    gcs = gcsfs.GCSFileSystem(token='anon')\n",
        "\n",
        "    all_regional_data = []\n",
        "\n",
        "    # Loop over each experiment\n",
        "    for exp in EXPERIMENTS:\n",
        "        print(f\"\\n--- Processing: {exp} ---\")\n",
        "        \n",
        "        # Find the zstore URL for this specific experiment and member_id\n",
        "        query = (\n",
        "            f\"source_id == '{SOURCE_ID}' & \"\n",
        "            f\"member_id == '{MEMBER_ID}' & \"\n",
        "            f\"experiment_id == '{exp}' & \"\n",
        "            f\"table_id == '{TABLE_ID}' & \"\n",
        "            f\"variable_id == '{VARIABLE_ID}'\"\n",
        "        )\n",
        "        \n",
        "        df_exp = df.query(query)\n",
        "        \n",
        "        if df_exp.empty:\n",
        "            print(f\"Warning: No data found for query: {query}\")\n",
        "            continue\n",
        "            \n",
        "        zstore = df_exp.iloc[0]['zstore']\n",
        "        print(f\"Found zstore: {zstore}\")\n",
        "\n",
        "        # Open the dataset\n",
        "        try:\n",
        "            mapper = gcs.get_mapper(zstore)\n",
        "            ds = xr.open_zarr(mapper, consolidated=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening zarr store {zstore}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Loop over each region\n",
        "        for region_name, region_box in REGIONS.items():\n",
        "            print(f\"Processing region: {region_name}\")\n",
        "            \n",
        "            try:\n",
        "                # 1. Select the region\n",
        "                ds_region = ds.sel(lat=region_box['lat'], lon=region_box['lon'])\n",
        "                \n",
        "                # 2. Create latitude weights for accurate averaging\n",
        "                weights = np.cos(np.deg2rad(ds_region.lat))\n",
        "                weights.name = 'weights'\n",
        "                \n",
        "                # 3. Calculate the weighted spatial mean\n",
        "                ds_weighted_mean = ds_region.weighted(weights).mean(dim=['lat', 'lon'])\n",
        "                \n",
        "                # 4. Select only July data (month == 7) as proxy for \"extreme\" heat\n",
        "                ds_july = ds_weighted_mean.sel(time=ds_weighted_mean.time.dt.month == 7)\n",
        "                \n",
        "                # 5. Convert from Kelvin to Celsius\n",
        "                temp_c = ds_july['tas'] - 273.15\n",
        "                \n",
        "                # 6. Trigger computation\n",
        "                print(\"Computing values...\")\n",
        "                temp_c_computed = temp_c.compute()\n",
        "                print(\"...computation complete.\")\n",
        "\n",
        "                # 7. Convert to Pandas DataFrame\n",
        "                df_temp = temp_c_computed.to_dataframe()\n",
        "                \n",
        "                if df_temp.empty:\n",
        "                    print(f\"Warning: No data after processing for {region_name}, {exp}\")\n",
        "                    continue\n",
        "\n",
        "                # 8. Clean up the DataFrame\n",
        "                df_temp['year'] = df_temp.index.year\n",
        "                df_temp = df_temp.reset_index(drop=True)[['year', 'tas']]\n",
        "                df_temp = df_temp.rename(columns={'tas': 'july_temp_c'})\n",
        "                df_temp['region'] = region_name\n",
        "                df_temp['scenario'] = exp\n",
        "                \n",
        "                # 9. Append to our master list\n",
        "                all_regional_data.append(df_temp)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {region_name} for {exp}: {e}\")\n",
        "\n",
        "        # Close the dataset\n",
        "        ds.close()\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    if all_regional_data:\n",
        "        print(\"\\n--- Concatenating all results ---\")\n",
        "        final_df = pd.concat(all_regional_data, ignore_index=True)\n",
        "        \n",
        "        # Save to CSV\n",
        "        output_filename = 'us_regional_july_temps.csv'\n",
        "        final_df.to_csv(output_filename, index=False)\n",
        "        \n",
        "        print(f\"\\n✅ Success! Data processed and saved to '{output_filename}'.\")\n",
        "        print(\"Final DataFrame head:\")\n",
        "        print(final_df.head())\n",
        "    else:\n",
        "        print(\"\\nNo data was processed. Output file not created.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found. Make sure it's in the same directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converts CSV to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Processing 'us_regional_july_temps.csv' for D3.js Map ---\n",
            "Loading '../data/us_regional_july_temps.csv'...\n",
            "CSV loaded successfully.\n",
            "Filtering data for years 2025-2100...\n",
            "Found 608 records in this date range.\n",
            "Rounded 'july_temp_c' to 2 decimal places.\n",
            "\n",
            "✅ Success! Data saved to 'us_temps_for_map_2025-2100.json'.\n",
            "This file is ready for your D3.js project.\n",
            "\n",
            "--- Example Data (first 10 records) ---\n",
            "[\n",
            "  {\n",
            "    \"year\":2025,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":24.28\n",
            "  },\n",
            "  {\n",
            "    \"year\":2026,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":24.45\n",
            "  },\n",
            "  {\n",
            "    \"year\":2027,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":22.34\n",
            "  },\n",
            "  {\n",
            "    \"year\":2028,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":22.18\n",
            "  },\n",
            "  {\n",
            "    \"year\":2029,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":22.91\n",
            "  },\n",
            "  {\n",
            "    \"year\":2030,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":22.21\n",
            "  },\n",
            "  {\n",
            "    \"year\":2031,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":24.84\n",
            "  },\n",
            "  {\n",
            "    \"year\":2032,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":23.74\n",
            "  },\n",
            "  {\n",
            "    \"year\":2033,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":22.84\n",
            "  },\n",
            "  {\n",
            "    \"year\":2034,\n",
            "    \"region\":\"Northeast\",\n",
            "    \"scenario\":\"ssp245\",\n",
            "    \"july_temp_c\":23.42\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Processing 'us_regional_july_temps.csv' for D3.js Map ---\")\n",
        "\n",
        "# Define file names\n",
        "input_csv = '../data/us_regional_july_temps.csv'\n",
        "output_json = 'us_temps_for_map_2025-2100.json'\n",
        "\n",
        "try:\n",
        "    # 1. Load the CSV file\n",
        "    print(f\"Loading '{input_csv}'...\")\n",
        "    df = pd.read_csv(input_csv)\n",
        "    print(\"CSV loaded successfully.\")\n",
        "\n",
        "    # 2. Filter data for the research question (2025-2100)\n",
        "    print(f\"Filtering data for years 2025-2100...\")\n",
        "    df_filtered = df[(df['year'] >= 2025) & (df['year'] <= 2100)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(\"\\nWarning: No data found for years 2025-2100.\")\n",
        "        print(\"Please check your 'us_regional_july_temps.csv' file.\")\n",
        "    else:\n",
        "        print(f\"Found {len(df_filtered)} records in this date range.\")\n",
        "\n",
        "        # 3. Round temperature for cleaner data\n",
        "        if 'july_temp_c' in df_filtered.columns:\n",
        "            df_filtered['july_temp_c'] = df_filtered['july_temp_c'].round(2)\n",
        "            print(\"Rounded 'july_temp_c' to 2 decimal places.\")\n",
        "        else:\n",
        "            print(\"\\nError: 'july_temp_c' column not found.\", file=sys.stderr)\n",
        "            sys.exit(1)\n",
        "            \n",
        "        # 4. Save to JSON in 'records' format\n",
        "        output_cols = ['year', 'region', 'scenario', 'july_temp_c']\n",
        "        df_final = df_filtered[output_cols]\n",
        "        df_final.to_json(output_json, orient='records')\n",
        "        \n",
        "        print(f\"\\n✅ Success! Data saved to '{output_json}'.\")\n",
        "        print(\"This file is ready for your D3.js project.\")\n",
        "        \n",
        "        print(\"\\n--- Example Data (first 10 records) ---\")\n",
        "        # Use .to_json() for a cleaner print that matches the file output\n",
        "        print(df_final.head(10).to_json(orient='records', indent=2))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: The file '{input_csv}' was not found.\", file=sys.stderr)\n",
        "except KeyError as e:\n",
        "    print(f\"\\nError: A required column is missing: {e}\", file=sys.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred: {e}\", file=sys.stderr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
